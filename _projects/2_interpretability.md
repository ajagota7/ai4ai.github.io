---
layout: page
title: Interpretability & Mechanistic Understanding
description: Understanding neural networks through geometric and mechanistic approaches
img: assets/img/7.jpg
importance: 2
category: research
related_publications: true
---

## Overview

Our interpretability research aims to understand the internal mechanisms and representations learned by neural networks. We combine tools from geometry, topology, and causal inference to make deep learning models more transparent and predictable.

## Key Research Questions

- What geometric structures emerge in the representation spaces of large language models?
- How can we identify and interpret individual circuits and mechanisms in neural networks?
- What role do different architectural components play in model behavior?
- How can we predict model behavior from internal representations?

## Current Projects

### Geometric Interpretability
Investigating the geometric properties of learned representations, including manifold structure, curvature, and symmetries in embedding spaces.

### Circuit Discovery
Developing automated methods for discovering interpretable computational circuits in transformer models, with applications to language understanding and generation.

### Activation Analysis
Studying activation patterns across layers and attention heads to understand information flow and feature composition in large models.

### Representation Probing
Designing probing classifiers and interventions to test hypotheses about what information is encoded in neural representations.

## Tools & Methods

- Mechanistic interpretability techniques
- Representation similarity analysis
- Causal intervention methods
- Visualization and attribution methods

---

*This is a template research area. Add your specific projects, publications, and experimental results.*

---
layout: page
title: AI Alignment & Safety
description: Developing frameworks for ensuring AI systems align with human values
img: assets/img/projects/ai_alignment.jpg
importance: 1
category: research
related_publications: true
---

## Overview

Our AI Alignment & Safety research focuses on developing robust frameworks and methodologies for ensuring AI systems behave in accordance with human values and intentions. We investigate both theoretical foundations and practical approaches to building aligned AI systems.

## Key Research Questions

- How can we formally specify and verify human values in AI systems?
- What are the fundamental challenges in maintaining alignment as AI systems become more capable?
- How can we design reward functions that capture complex human preferences?
- What mechanisms can prevent specification gaming and reward hacking?

## Current Projects

### Value Alignment Framework
Developing a comprehensive framework for representing and reasoning about human values in machine learning systems. This includes work on preference learning, value extrapolation, and robustness to distributional shift.

### Scalable Oversight Methods
Investigating techniques for maintaining human oversight of increasingly capable AI systems, including debate, amplification, and recursive reward modeling approaches.

### Safety Benchmarks
Creating standardized benchmarks and evaluation metrics for assessing AI safety properties in foundation models.

## Collaborations

We actively collaborate with leading AI safety organizations and participate in workshops on AI alignment and existential risk mitigation.

---

*This is a template research area. Add your specific projects, publications, and results as you progress in your research.*

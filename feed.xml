<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ajagota7.github.io/ai4ai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ajagota7.github.io/ai4ai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-13T17:59:57+00:00</updated><id>https://ajagota7.github.io/ai4ai.github.io/feed.xml</id><title type="html">Sonali Parbhoo</title><subtitle>AI for Actionable Impact Lab - Developing trustworthy AI systems for healthcare through causal inference, reinforcement learning, and interpretability. </subtitle><entry><title type="html">The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</title><link href="https://ajagota7.github.io/ai4ai.github.io/blog/2025/alignment-auditor/" rel="alternate" type="text/html" title="The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives"/><published>2025-10-09T06:41:00+00:00</published><updated>2025-10-09T06:41:00+00:00</updated><id>https://ajagota7.github.io/ai4ai.github.io/blog/2025/alignment-auditor</id><content type="html" xml:base="https://ajagota7.github.io/ai4ai.github.io/blog/2025/alignment-auditor/"><![CDATA[<p>We‚Äôre thrilled to share our new paper, <strong>‚ÄúThe Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives‚Äù</strong>! We introduce a novel framework for auditing the alignment of Large Language Models.</p> <h2 id="our-three-stage-audit-framework">Our Three-Stage Audit Framework</h2> <p>Our framework provides a comprehensive approach to LLM alignment through three key stages:</p> <ol> <li><strong>Recovering a posterior distribution</strong> over reward functions to quantify ambiguity</li> <li><strong>Auditing trustworthiness</strong> via uncertainty diagnostics</li> <li><strong>Policy-level validation</strong> of the inferred reward, reframing reward inference as a complete audit rather than mere estimation</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/ai4ai.github.io/assets/img/alignment-auditor-framework-480.webp 480w,/ai4ai.github.io/assets/img/alignment-auditor-framework-800.webp 800w,/ai4ai.github.io/assets/img/alignment-auditor-framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/ai4ai.github.io/assets/img/alignment-auditor-framework.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Our three-stage framework: (1) Posterior recovery using variational inference, (2) Uncertainty diagnostics through iterative posterior contraction, (3) Policy-level reward validation via RLHF </div> <h2 id="key-finding-1-systematic-ambiguity-reduction">Key Finding 1: Systematic Ambiguity Reduction</h2> <p>We demonstrate that sequential updates contract the reward posterior, yielding <strong>steady AUROC and accuracy gains</strong> on Llama-1B over 5 rounds. This provides clear evidence that we‚Äôre converging on the true objective function.</p> <p>The iterative refinement process shows:</p> <ul> <li>Consistent improvement in reward prediction accuracy</li> <li>Reduced uncertainty in posterior estimates</li> <li>Better alignment between inferred and true objectives</li> </ul> <h2 id="key-finding-2-trustworthiness-diagnostics">Key Finding 2: Trustworthiness Diagnostics</h2> <p>Our uncertainty diagnostics successfully <strong>flag untrustworthy inputs</strong>. We find that reward uncertainty correlates strongly with:</p> <ul> <li>Out-of-distribution examples</li> <li>Ambiguous preference comparisons</li> <li>Cases where the model‚Äôs predictions are unreliable</li> </ul> <p>This correlation allows us to identify when the auditor‚Äôs judgments should be treated with caution, providing crucial safety guarantees for deployment.</p> <h2 id="implications-for-ai-safety">Implications for AI Safety</h2> <p>This work represents a significant step toward making LLM alignment more:</p> <ul> <li><strong>Transparent</strong>: By explicitly modeling uncertainty in the reward function</li> <li><strong>Auditable</strong>: Through systematic validation at multiple levels</li> <li><strong>Reliable</strong>: By flagging cases where alignment may be uncertain</li> </ul> <p>By treating alignment as an auditing problem rather than just an estimation task, we can build more trustworthy AI systems.</p> <hr/> <p><strong>Read the full paper</strong>: [arXiv link coming soon]</p> <p><strong>Tags</strong>: #AI #LLM #AISafety #Alignment #BayesianMethods</p>]]></content><author><name></name></author><category term="research-updates"/><category term="alignment"/><category term="ai-safety"/><category term="llm"/><category term="bayesian-methods"/><summary type="html"><![CDATA[Introducing a new framework for auditing the alignment of Large Language Models through Bayesian inference]]></summary></entry><entry><title type="html">Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</title><link href="https://ajagota7.github.io/ai4ai.github.io/blog/2025/ards-diagnosis-cbm/" rel="alternate" type="text/html" title="Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"/><published>2025-08-15T10:00:00+00:00</published><updated>2025-08-15T10:00:00+00:00</updated><id>https://ajagota7.github.io/ai4ai.github.io/blog/2025/ards-diagnosis-cbm</id><content type="html" xml:base="https://ajagota7.github.io/ai4ai.github.io/blog/2025/ards-diagnosis-cbm/"><![CDATA[<p>Happy to share our latest work presented at <strong>#MLHC2025</strong>!</p> <h2 id="context-aware-concept-bottleneck-models-for-ards">Context-Aware Concept Bottleneck Models for ARDS</h2> <p>üß† <strong>Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</strong></p> <p>Large, publicly available clinical datasets have emerged as a novel resource for understanding disease heterogeneity and exploring personalization of therapy. Our work leverages these datasets to improve diagnosis of Acute Respiratory Distress Syndrome (ARDS).</p> <h3 id="key-innovation">Key Innovation</h3> <p>‚ú® <strong>Combines multiple data modalities:</strong></p> <ul> <li>Structured ICU data (vital signs, lab values)</li> <li>LLM-extracted concepts from clinical notes</li> <li>Information from medical imaging</li> </ul> <h3 id="results">Results</h3> <p>üìà <strong>Achieves 8-10% accuracy improvement</strong> without compromising interpretability</p> <p>By using concept bottleneck models, we maintain the ability to understand <em>why</em> the model makes its predictions, which is crucial for clinical adoption and trust.</p> <h3 id="why-this-matters">Why This Matters</h3> <p>ARDS is a critical condition that requires timely and accurate diagnosis. Our approach demonstrates that:</p> <ul> <li>Combining structured and unstructured clinical data improves predictions</li> <li>Interpretable AI models can achieve high performance</li> <li>LLMs can extract meaningful concepts from clinical text</li> </ul> <hr/> <p>üìÑ <strong>Read the full paper</strong>: <a href="https://arxiv.org/abs/2508.09719">arXiv:2508.09719</a></p> <p><strong>Conference</strong>: Machine Learning for Healthcare (MLHC) 2025</p>]]></content><author><name></name></author><category term="research-updates"/><category term="healthcare"/><category term="concept-bottleneck-models"/><category term="interpretability"/><category term="clinical-ai"/><summary type="html"><![CDATA[Combining ICU data with LLM-extracted concepts from clinical notes and imaging to improve ARDS diagnosis accuracy]]></summary></entry><entry><title type="html">Concept-Driven Off-Policy Evaluation</title><link href="https://ajagota7.github.io/ai4ai.github.io/blog/2025/concept-driven-ope/" rel="alternate" type="text/html" title="Concept-Driven Off-Policy Evaluation"/><published>2025-08-04T09:00:00+00:00</published><updated>2025-08-04T09:00:00+00:00</updated><id>https://ajagota7.github.io/ai4ai.github.io/blog/2025/concept-driven-ope</id><content type="html" xml:base="https://ajagota7.github.io/ai4ai.github.io/blog/2025/concept-driven-ope/"><![CDATA[<p>Excited to share that our paper <strong>‚ÄúConcept-Driven Off-Policy Evaluation‚Äù</strong> is being presented at <strong>#RLC2025</strong> this week!</p> <h2 id="rethinking-off-policy-evaluation">Rethinking Off-Policy Evaluation</h2> <p>We introduce a novel approach to off-policy evaluation (OPE) that uses <strong>interpretable concepts</strong> rather than raw features.</p> <h3 id="why-concept-driven-ope">Why Concept-Driven OPE?</h3> <p>Traditional OPE methods often suffer from:</p> <ul> <li>High variance in value estimates</li> <li>Black-box evaluation processes</li> <li>Difficulty understanding what drives policy performance</li> </ul> <p>Our concept-driven approach addresses these challenges by:</p> <h3 id="key-contributions">Key Contributions</h3> <p>üéØ <strong>Low-variance estimation</strong>: By operating at the concept level, we reduce the dimensionality and variance of OPE estimates</p> <p>üîç <strong>Transparency</strong>: Concepts provide interpretable explanations for why one policy outperforms another</p> <p>‚ö° <strong>Actionable insights</strong>: Understanding performance at the concept level enables targeted policy improvements</p> <h3 id="applications">Applications</h3> <p>This work has important implications for:</p> <ul> <li>Healthcare: Evaluating treatment policies with interpretable feedback</li> <li>Robotics: Understanding which perceptual concepts matter for task success</li> <li>Autonomous systems: Auditing policies before deployment</li> </ul> <h3 id="technical-approach">Technical Approach</h3> <p>We leverage causal inference to:</p> <ol> <li>Identify relevant concepts for policy evaluation</li> <li>Construct low-variance OPE estimators using concept-level importance sampling</li> <li>Provide concept-level explanations for policy differences</li> </ol> <hr/> <p>üìÑ <strong>Read the full paper</strong>: <a href="https://arxiv.org/abs/2411.19395">arXiv:2411.19395</a></p> <p><strong>Conference</strong>: Reinforcement Learning Conference (RLC) 2025</p> <p><strong>Tags</strong>: #RL #CausalAI #Interpretability #OffPolicyEvaluation</p>]]></content><author><name></name></author><category term="research-updates"/><category term="reinforcement-learning"/><category term="off-policy-evaluation"/><category term="interpretability"/><category term="causal-ai"/><summary type="html"><![CDATA[Rethinking OPE using interpretable concepts for low-variance, transparent, and actionable evaluation]]></summary></entry></feed>